#
### Namespace
#
---
apiVersion: v1
kind: Namespace
metadata:
  name: $NAMESPACE

#
### OpenObserve
#
---
apiVersion: v1
kind: Service
metadata:
  name: $SERVICE
  namespace: $NAMESPACE
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: $SERVICE
    app.kubernetes.io/part-of: $SERVICE
  ports:
    - name: http
      port: 80
      targetPort: 5080
      protocol: TCP
---
apiVersion: v1
kind: Secret
metadata:
  name: $SERVICE-root
  namespace: $NAMESPACE
data:
  password: $BASE64_ROOT_OBSERVE_PASSWORD
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: $SERVICE
  namespace: $NAMESPACE
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    acme.cert-manager.io/http01-edit-in-place: "true"
spec:
  tls:
    - hosts:
        - $MANAGED_DOMAIN
      secretName: $SERVICE-tls-certificate
  rules:
    - host: $MANAGED_DOMAIN
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: $SERVICE
                port:
                  number: 80
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: $SERVICE
  namespace: $NAMESPACE
  labels:
    name: $SERVICE
spec:
  serviceName: $SERVICE
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: $SERVICE
      app.kubernetes.io/part-of: $SERVICE
  template:
    metadata:
      labels:
        app.kubernetes.io/name: $SERVICE
        app.kubernetes.io/part-of: $SERVICE
    spec:
      securityContext:
        fsGroup: 2000
        runAsUser: 10000
        runAsGroup: 3000
        runAsNonRoot: true
      containers:
        - name: openobserve
          image: public.ecr.aws/zinclabs/openobserve:latest
          env:
            - name: ZO_ROOT_USER_EMAIL
              value: $OBSERVE_ROOT_USER_EMAIL
            - name: ZO_ROOT_USER_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: $SERVICE-root
                  key: password
            - name: ZO_DATA_DIR
              value: /data
            - name: ZO_COMPACT_DATA_RETENTION_DAYS
              value: "30"
            - name: ZO_METRICS_LEADER_PUSH_INTERVAL
              value: "30"
          # command: ["/bin/bash", "-c", "while true; do sleep 1; done"]
          imagePullPolicy: Always
          resources:
            limits:
              cpu: 4096m
              memory: 2048Mi
            requests:
              cpu: 256m
              memory: 50Mi
          ports:
            - containerPort: 5080
              name: http
          volumeMounts:
            - name: data
              mountPath: /data
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
#
### OpenObserve OTEL Collector
#
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: openobserve-collector
  namespace: $NAMESPACE
  labels:
    app: openobserve-collector
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: openobserve-collector
  labels:
    app: openobserve-collector
rules:
  - nonResourceURLs: ["/metrics", "/metrics/cadvisor"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources:
      - configmaps
      - endpoints
      - events
      - namespaces
      - namespaces/status
      - nodes
      - nodes/spec
      - nodes/stats
      - nodes/metrics
      - nodes/proxy
      - persistentvolumes
      - persistentvolumeclaims
      - pods
      - pods/status
      - replicationcontrollers
      - replicationcontrollers/status
      - resourcequotas
      - services
    verbs: ["get", "list", "watch"]
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - servicemonitors
      - podmonitors
      - probes
      - scrapeconfigs
    verbs: ["*"]
  - apiGroups: ["apps"]
    resources:
      - daemonsets
      - deployments
      - replicasets
      - statefulsets
    verbs: ["get", "list", "watch"]
  - apiGroups: ["extensions"]
    resources:
      - ingresses
    verbs: ["get", "list", "watch"]
  - apiGroups: ["batch"]
    resources:
      - jobs
      - cronjobs
    verbs: ["get", "list", "watch"]
  - apiGroups: ["autoscaling"]
    resources:
      - horizontalpodautoscalers
    verbs: ["get", "list", "watch"]
  - apiGroups: ["networking.k8s.io"]
    resources:
      - ingresses
    verbs: ["get", "list", "watch"]
  - apiGroups: ["discovery.k8s.io"]
    resources:
      - endpointslices
    verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: openobserve-collector
  labels:
    app: openobserve-collector
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: openobserve-collector
subjects:
  - kind: ServiceAccount
    name: openobserve-collector
    namespace: $NAMESPACE
---
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: openobserve-collector-gateway
  namespace: $NAMESPACE
  labels:
    app: openobserve-collector
spec:
  mode: statefulset
  targetAllocator:
    enabled: true
    serviceAccount: openobserve-collector
    prometheusCR:
      enabled: true
  serviceAccount: openobserve-collector
  image: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:0.113.0
  resources: {}
  config:
    receivers:
      otlp:
        protocols:
          grpc: {}
          http: {}
      k8s_cluster:
        collection_interval: 30s
        node_conditions_to_report:
          [Ready, MemoryPressure, DiskPressure, PIDPressure]
        allocatable_types_to_report: [cpu, memory, storage]
        metrics:
          k8s.container.cpu_limit: # redundant
            enabled: false
          k8s.container.cpu_request: # redundant
            enabled: false
          k8s.container.memory_limit: # redundant
            enabled: false
          k8s.container.memory_request: # redundant
            enabled: false
      k8s_events:
        auth_type: serviceAccount
      k8sobjects:
        auth_type: serviceAccount
        objects:
          - name: pods
            mode: pull
            # label_selector: environment in (production),tier in (frontend)
            field_selector: status.phase=Running
            interval: 15m
          - name: events
            mode: watch
            group: events.k8s.io
      prometheus:
        config:
          scrape_configs: []
    connectors:
      spanmetrics:
        histogram:
          explicit:
            buckets:
              [
                100us,
                1ms,
                2ms,
                6ms,
                10ms,
                100ms,
                250ms,
                500ms,
                1000ms,
                1400ms,
                2000ms,
                5s,
                10s,
                30s,
                60s,
                120s,
                300s,
                600s,
              ]
        dimensions:
          - name: http.method
            default: GET
          - name: http.status_code
        exemplars:
          enabled: true
        dimensions_cache_size: 1000
        aggregation_temporality: "AGGREGATION_TEMPORALITY_CUMULATIVE"
        metrics_flush_interval: 15s
      servicegraph:
        latency_histogram_buckets: [1, 2, 3, 4, 5]
        dimensions:
          - http.method
        store:
          ttl: 1s
          max_items: 10
    processors:
      attributes:
        actions:
          - key: k8s_cluster
            action: insert
            value: cluster
      resourcedetection:
        detectors: [env]
        override: true
        timeout: 2s
      k8sattributes:
        auth_type: "serviceAccount"
        passthrough: false
        extract:
          labels:
            - tag_name: service.name
              key: app.kubernetes.io/name
              from: pod
            - tag_name: service.name
              key: k8s-app
              from: pod
            - tag_name: k8s.app.instance
              key: app.kubernetes.io/instance
              from: pod
            - tag_name: service.version
              key: app.kubernetes.io/version
              from: pod
            - tag_name: k8s.app.component
              key: app.kubernetes.io/component
              from: pod
          metadata:
            - k8s.namespace.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.node.name
            - k8s.pod.start_time
            - k8s.deployment.name
            - k8s.replicaset.name
            - k8s.replicaset.uid
            - k8s.daemonset.name
            - k8s.daemonset.uid
            - k8s.job.name
            - k8s.job.uid
            - k8s.container.name
            - k8s.cronjob.name
            - k8s.statefulset.name
            - k8s.statefulset.uid
            - container.image.tag
            - container.image.name
            - k8s.cluster.uid
        pod_association:
          - sources:
              - from: resource_attribute
                name: k8s.pod.uid
          - sources:
              - from: resource_attribute
                name: k8s.pod.name
              - from: resource_attribute
                name: k8s.namespace.name
              - from: resource_attribute
                name: k8s.node.name
          - sources:
              - from: resource_attribute
                name: k8s.pod.ip
          - sources:
              - from: resource_attribute
                name: k8s.pod.name
              - from: resource_attribute
                name: k8s.namespace.name
          - sources:
              - from: connection
      batch:
        send_batch_size: 10000
        timeout: 10s
    extensions:
      zpages: {}
    exporters:
      otlphttp/openobserve:
        endpoint: http://observe.runtime.svc.cluster.local/api/default/
        headers:
          Authorization: Basic $BASE64_OBSERVE_TOKEN
      otlphttp/openobserve_k8s_events:
        endpoint: http://observe.runtime.svc.cluster.local/api/default/
        headers:
          Authorization: Basic $BASE64_OBSERVE_TOKEN
          stream-name: k8s_events
    service:
      extensions: [zpages]
      pipelines:
        logs/k8s_events:
          receivers: [k8s_events]
          processors: [batch, attributes, k8sattributes, resourcedetection]
          exporters: [otlphttp/openobserve_k8s_events]
        metrics:
          receivers: [k8s_cluster, spanmetrics, servicegraph]
          processors: [batch, attributes, k8sattributes, resourcedetection]
          exporters: [otlphttp/openobserve]
        traces:
          receivers: [otlp]
          processors: [batch, attributes, k8sattributes, resourcedetection]
          exporters: [otlphttp/openobserve, spanmetrics, servicegraph]
---
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: openobserve-collector-agent
  namespace: $NAMESPACE
  labels:
    app: openobserve-collector
spec:
  mode: daemonset # "daemonset", "deployment" (default), "statefulset"
  serviceAccount: openobserve-collector
  image: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:0.113.0
  env:
    - name: K8S_NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: K8S_NODE_IP
      valueFrom:
        fieldRef:
          fieldPath: status.hostIP
  volumes:
    - name: hostfs
      hostPath:
        path: /
    - name: varlog
      hostPath:
        path: /var/log
        type: ""
    - name: varlibdockercontainers
      hostPath:
        path: /var/lib/docker/containers
        type: ""
  volumeMounts:
    - name: hostfs
      mountPath: /hostfs
    - name: varlog
      mountPath: /var/log
    - name: varlibdockercontainers
      readOnly: true
      mountPath: /var/lib/docker/containers
  resources: {}
  config:
    receivers:
      otlp:
        protocols:
          grpc: {}
          http: {}
      prometheus:
        config:
          scrape_configs:
            - job_name: "otel-collector"
              scrape_interval: 5s
              static_configs:
                - targets: ["0.0.0.0:8888"]
      filelog/std:
        include: [/var/log/pods/*/*/*.log]
        exclude:
          # Exclude logs from all containers named otel-collector or otc-container (otel-contrib)
          - /var/log/pods/*/otel-collector/*.log # named otel-collector
          - /var/log/pods/*/otc-container/*.log # named otc-container (for otel-contrib containers)
          - /var/log/pods/*/openobserve-ingester/*.log # avoid cyclical logs as ingester logs can be massive
        start_at: end
        include_file_path: true
        include_file_name: false
        operators:
          # Find out which format is used by kubernetes
          - type: router
            id: get-format
            routes:
              - output: parser-docker
                expr: 'body matches "^\\{"'
              - output: parser-crio
                expr: 'body matches "^[^ Z]+ "'
              - output: parser-containerd
                expr: 'body matches "^[^ Z]+Z"'
          # Parse CRI-O format
          - type: regex_parser
            id: parser-crio
            regex: "^(?P<time>[^ Z]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$"
            output: extract_metadata_from_filepath
            timestamp:
              parse_from: attributes.time
              layout_type: gotime
              layout: "2006-01-02T15:04:05.999999999Z07:00"
          # Parse CRI-Containerd format
          - type: regex_parser
            id: parser-containerd
            regex: "^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$"
            output: extract_metadata_from_filepath
            timestamp:
              parse_from: attributes.time
              layout: "%Y-%m-%dT%H:%M:%S.%LZ"
          # Parse Docker format
          - type: json_parser
            id: parser-docker
            output: extract_metadata_from_filepath
            timestamp:
              parse_from: attributes.time
              layout: "%Y-%m-%dT%H:%M:%S.%LZ"
          # Extract metadata from file path
          - type: regex_parser
            id: extract_metadata_from_filepath
            regex: '^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]{36})\/(?P<container_name>[^\._]+)\/(?P<restart_count>\d+)\.log$'
            parse_from: attributes["log.file.path"]
            cache:
              size: 128 # default maximum amount of Pods per Node is 110
          # Update body field after finishing all parsing
          - type: move
            from: attributes.log
            to: body
          # Rename attributes
          - type: move
            from: attributes.stream
            to: attributes["log.iostream"]
          - type: move
            from: attributes.container_name
            to: resource["k8s.container.name"]
          - type: move
            from: attributes.namespace
            to: resource["k8s.namespace.name"]
          - type: move
            from: attributes.pod_name
            to: resource["k8s.pod.name"]
          - type: move
            from: attributes.restart_count
            to: resource["k8s.container.restart_count"]
          - type: move
            from: attributes.uid
            to: resource["k8s.pod.uid"]
      hostmetrics:
        root_path: /hostfs
        collection_interval: 30s
        scrapers:
          cpu: {}
          disk: {}
          filesystem:
            exclude_mount_points:
              match_type: regexp
              mount_points:
                - /dev/.*
                - /proc/.*
                - /sys/.*
                - /run/k3s/containerd/.*
                - /var/lib/docker/.*
                - /var/lib/kubelet/.*
                - /snap/.*
            exclude_fs_types:
              match_type: strict
              fs_types:
                - autofs
                - binfmt_misc
                - bpf
                - cgroup2
                - configfs
                - debugfs
                - devpts
                - devtmpfs
                - fusectl
                - hugetlbfs
                - iso9660
                - mqueue
                - nsfs
                - overlay
                - proc
                - procfs
                - pstore
                - rpc_pipefs
                - securityfs
                - selinuxfs
                - squashfs
                - sysfs
                - tracefs
          load: {}
          network: {}
      kubeletstats:
        collection_interval: 30s
        auth_type: "serviceAccount"
        endpoint: "https://${env:K8S_NODE_IP}:10250"
        insecure_skip_verify: true
        extra_metadata_labels:
          - container.id
          - k8s.volume.type
        metric_groups:
          - node
          - pod
          - container
          - volume
        metrics:
          k8s.pod.cpu_limit_utilization:
            enabled: true
          k8s.pod.cpu_request_utilization:
            enabled: true
          k8s.pod.memory_limit_utilization:
            enabled: true
          k8s.pod.memory_request_utilization:
            enabled: true
    connectors: {}
    processors:
      attributes:
        actions:
          - key: k8s_cluster
            action: insert
            value: cluster
      resourcedetection:
        detectors: [system, env, k8snode]
        override: true
        system:
          hostname_sources: [os, dns]
      k8sattributes:
        auth_type: "serviceAccount"
        passthrough: false
        filter:
          node_from_env_var: K8S_NODE_NAME
        extract:
          labels:
            - tag_name: service.name
              key: app.kubernetes.io/name
              from: pod
            - tag_name: service.name
              key: k8s-app
              from: pod
            - tag_name: k8s.app.instance
              key: app.kubernetes.io/instance
              from: pod
            - tag_name: service.version
              key: app.kubernetes.io/version
              from: pod
            - tag_name: k8s.app.component
              key: app.kubernetes.io/component
              from: pod
          metadata:
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.deployment.name
            - k8s.namespace.name
            - k8s.node.name
            - k8s.pod.start_time
        pod_association:
          - sources:
              - from: resource_attribute
                name: k8s.pod.uid
          - sources:
              - from: resource_attribute
                name: k8s.pod.name
              - from: resource_attribute
                name: k8s.namespace.name
              - from: resource_attribute
                name: k8s.node.name
          - sources:
              - from: resource_attribute
                name: k8s.pod.ip
          - sources:
              - from: resource_attribute
                name: k8s.pod.name
              - from: resource_attribute
                name: k8s.namespace.name
          - sources:
              - from: connection
      batch:
        send_batch_size: 10000
        timeout: 10s
    extensions:
      zpages: {}
    exporters:
      otlphttp/openobserve:
        endpoint: http://observe.runtime.svc.cluster.local/api/default/
        headers:
          Authorization: Basic $BASE64_OBSERVE_TOKEN
      otlphttp/openobserve_k8s_events:
        endpoint: http://observe.runtime.svc.cluster.local/api/default/
        headers:
          Authorization: Basic $BASE64_OBSERVE_TOKEN
          stream-name: k8s_events
    service:
      extensions: [zpages]
      pipelines:
        logs:
          receivers: [filelog/std]
          processors: [batch, attributes, k8sattributes]
          exporters: [otlphttp/openobserve]
        metrics:
          receivers: [kubeletstats]
          processors: [batch, attributes, k8sattributes]
          exporters: [otlphttp/openobserve]
        traces:
          receivers: [otlp]
          processors: [batch, attributes, k8sattributes]
          exporters: [otlphttp/openobserve]
